# HMM Transformer Model

An attempt to replicate the Shai et alia 2025 paper "Transformers Represent Belief-State Geometry In Their Residual Streams".  Written with aid from Claude 3.7, though I corrected most of the math - this two-hands approach may have resulted in some (minimal) disharmony between comments and function.  It doesn't quite work to reproduce the results of the paper at the small number of epochs for which I trained (a few thousand on colab, compared to the paper's 1M - though I used Adam and they SGD).  If undertraining is not the problem (and I suspect it is not), I do not know what or where the presumptive bug is.

This project implements a Hidden Markov Model (HMM) with a transformer model that has 64-dimensional residual connections. The transformer is trained to predict the next token in sequences generated by the HMM.  The code contains functionalities to determine Bayes-optimal belief states for the HMM given a sequence of token observations, and uses these to fit a linear subspace onto which the transformers' last-layer activations are projected, with the intent to replicate the result of Shai et alia 2025, showing that these projections converge over training to a linear representation of the Bayes-optimal beliefs, even for complex processes with fractal belief-state geometry like Mess3.

## Setup

1. Install dependencies:
```
pip install -r requirements.txt
```

2. Run the model:
```
python hmm_transformer.py
```

## Model Architecture (OLD, likely not the same as is now in the code)

- **HMM Model**: A 3-state Hidden Markov Model with parameters x=0.15 and alpha=0.6
- **Transformer**: A transformer model with 64-dimensional embeddings and residual connections
  - 4 attention heads
  - 3 transformer layers
  - 256-dimensional feedforward network
  - Dropout of 0.1

## Training

The model is trained to predict the next token in sequences generated by the HMM. Training parameters:
- Batch size: 32
- Sequence length: 100
- Learning rate: 0.001
- Number of epochs: 10
